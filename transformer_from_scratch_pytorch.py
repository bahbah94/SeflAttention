# -*- coding: utf-8 -*-
"""transformer_from_scratch_pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_8qHXPZMsTzbAk0-6nS-ypbov1_o-TY0
"""

import pandas as pd
import statistics
import numpy as np
import os
import matplotlib.pyplot as plt
#import optuna
import torch
import torch.nn.functional as F
from torchsummary import summary
from tqdm import tqdm
#import transformers
import torch.nn as nn
from torch.utils.data import Dataset , DataLoader , RandomSampler, SequentialSampler
#from transformers import BertForSequenceClassification , AdamW
import time
#from transformers import get_linear_schedule_with_warmup
from sklearn.metrics import f1_score , precision_score, confusion_matrix
from torch import optim

class SelfAttention(nn.Module):
  def __init__(self,k,heads):
    self.k = k # feature size after preprocessing
    self.heads = heads # number of self attention heads
    self.keys = nn.Linear(k,k*heads,bias=True) #matrix to be multiplied with x_i 
    self.queries = nn.Linear(k,k*heads,bias=True)  # matrix to be again multiplied with x_i. Both keys and queries will give our weight
    self.values = nn.Linear(k,k*heads,bias=True) #this will be multiplied with weight and summed to get the output for attention head
    self.unify = nn.Linear()

  def forward(self,x):
    b,t,k = x.size()
    h = self.heads

    keys = self.keys(x).view(b,h,t,k)
    queries = self.queries(x).view(b,h,t,k)
    values = self.values(x).view(b,h,t,k)

    keys = torch.transpose(keys,1,2).contiguous().view(b*h,t,k)
    queries = torch.transpose(queries,1,2).contiguous().view(b*h,t,k)
    values = torch.transpose(values,1,2).continuous().view(b*h,t,k)

    queries = queries / (k ** (1/4))
    keys    = keys / (k ** (1/4))

    dot = torch.bmm(keys,queries.transpose(queries,1,2)) #dimension of (b*h,t,t)
    dot = torch.softmax(dot,dim=2)

    final = torch.bmm(dot,values).view(b,h,t,k)

    out = torch.transpose(final,1, 2).contiguous().view(b, t, h * k)
    return self.unifyheads(out)

"""Now for the transformer Block"""

class Transformer(nn.Module):
  def __init__(self,k,heads=8):
    super().__init__()
    
    self.attention = SelfAttention(k, heads=heads)

    self.norm1 = nn.LayerNorm(k)
    self.norm2 = nn.LayerNorm(k)

    self.ff = nn.Sequential(
      nn.Linear(k, 8 * k),
      nn.ReLU(),
      nn.Linear(8 * k, k))

  def forward(self, x):
    attended = self.attention(x)
    x = self.norm1(attended + x)

    fedforward = self.ff(x)
    return self.norm2(fedforward + x)